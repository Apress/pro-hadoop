package com.apress.hadoopbook.utils;

import java.io.IOException;
import java.util.Formatter;
import java.util.Random;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

/** Simple class to provide a way of rapidly generating data files for input. */
public abstract class GenerateDataFile {
	
	/** This method returns a filename to use for this file.
	 * 
	 * @param conf The Job Conf object to use
	 * @param start The starting sequence number
	 * @param count The number of lines to generate
	 * @return A file name.
	 */
	public abstract Path getFileName(Configuration conf, int start, int count);
	
	/** Generate an output line.
	 * 
	 * This method should generate one or more lines of output.
	 * 
	 * @param conf The configuration object to use
	 * @param fmt A {@link Formatter} object tied to the output file.
	 * @param ordinal The ordinal number of this call to outputLine
	 * @param seq The sequence number of this record. These are random
	 * @param start The starting sequence number.
	 * @param end The final sequence number
	 * @param filename The filename being written to
	 * @param out The raw output data stream.
	 */
	public abstract void outputLine( Configuration conf, Formatter fmt, int ordinal, int seq, int start,
			int end, Path filename, FSDataOutputStream out);
	
	/** Generate a datafile of <code>count</code> lines, starting with with <code>start</code>.
	 * The output lines are randomized.
	 * 
	 * The filename comes from the {@link #getFileName(Configuration, int, int)} method.
	 * Each line is generated by the {@link #outputLine(Configuration, Formatter, int, int, int, int, Path, FSDataOutputStream)}.
	 * 
	 * @param conf The configuration to use
	 * @param start The starting sequence number
	 * @param count The number of lines to write
	 * @return The path of the file generated.
	 * @throws IOException
	 */
	public Path generateDataFile( final Configuration conf, final int start, final int count ) throws IOException
	{
		/** Simple sanity check */
		if (start<0||count<=0) {
			throw new IOException( "Invalid start " + start + " or count " + count );
		}
		
		final Path fileName = getFileName(conf, start, count);
		final FileSystem fs = fileName.getFileSystem(conf);
		
		/** Create the file and fail if the result is odd */
		FSDataOutputStream out = null;
		
		try {
			out = fs.create(fileName);
			if (out==null) {
				throw new IOException( "Unable to create output file " + fileName);
			}
			/** Create an output format object that will write to the newly created file */
			final Formatter fmt = new Formatter(out);
			/** Write the output data */
			final int end = start + count;
			/** Build an array of indices and randomize them to demonstrate reduce sorting */
			int[] indices = new int[count];
			/** Fill the indices array with ordered initial values */
			for( int i = 0; i < count; i++ ) {
				indices[i] = start + i;
			}
			final Random r = new Random();
			int tmpVal;
			int randomIndex;
			/** Like the (@link java.util.Collections#shuffle()) .*/
			for (int i= count; i>1; i--) {
				tmpVal = indices[i-1];
				randomIndex = r.nextInt(i);
				indices[i-1] = indices[randomIndex];
				indices[randomIndex] = tmpVal;
			}
			for ( int i = 0; i < count; i++ ) {
				outputLine( conf, fmt, i, indices[i], start, end, fileName, out);
			}
			/** Force the output out to the descriptor in case there is some buffering */
			fmt.flush();
			out.close();
			out = null;
			return fileName;
		} finally {
			/** It is especially important in Hadoop to ensure that any objects that hold file descriptors are closed.
			 * at least through Hadoop 0.19.0, sync/flush has no effect and blocks are written to HDFS ONLY when a full filesystem blocksize worth of data has been written or the file is closed.
			 * There are also numerous hard to diagnose failure cases from running out of file descriptors.
			 */
			if (out!=null) {
				out.close();
			}
		}

	}

}
